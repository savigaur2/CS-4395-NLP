{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Portfolio Assingment 3: Exploring NLTK"
      ],
      "metadata": {
        "id": "4qtvxbcEaRp4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBlYvbPTaARK",
        "outputId": "cb5eb7f7-24e3-40c2-b86a-11981c3cc772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('book')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY9OG_3Pg-wZ",
        "outputId": "adccc564-bd15-499f-ade5-15e771c2ffef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Extract the first 20 tokens from text1\n",
        "\n",
        "### Two things I learned about tokens() or Text objects:\n",
        "1. Text objects are initialized with tokens and an optional name as parameters\n",
        "2. Using the TokenSearcher, we can use regex to search over tokenized strings"
      ],
      "metadata": {
        "id": "kLddhGMTg6LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.book"
      ],
      "metadata": {
        "id": "CMSFApV5krIa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.book.text1.tokens[0:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5gpg3efapwz",
        "outputId": "113795ed-6367-4e43-b21a-266b4dbc6208"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'Moby',\n",
              " 'Dick',\n",
              " 'by',\n",
              " 'Herman',\n",
              " 'Melville',\n",
              " '1851',\n",
              " ']',\n",
              " 'ETYMOLOGY',\n",
              " '.',\n",
              " '(',\n",
              " 'Supplied',\n",
              " 'by',\n",
              " 'a',\n",
              " 'Late',\n",
              " 'Consumptive',\n",
              " 'Usher',\n",
              " 'to',\n",
              " 'a',\n",
              " 'Grammar']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Print a concordance for text1 word 'sea', selecting only 5 lines. "
      ],
      "metadata": {
        "id": "0DYKIqWvlL8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.book.text1.concordance('sea', lines = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLCOFM2uh7fd",
        "outputId": "80d0f031-9fd6-4a32-da65-22478127905c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 5 of 455 matches:\n",
            " shall slay the dragon that is in the sea .\" -- ISAIAH \" And what thing soever \n",
            " S PLUTARCH ' S MORALS . \" The Indian Sea breedeth the most and the biggest fis\n",
            "cely had we proceeded two days on the sea , when about sunrise a great many Wha\n",
            "many Whales and other monsters of the sea , appeared . Among the former , one w\n",
            " waves on all sides , and beating the sea before him into a foam .\" -- TOOKE ' \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Experiment with Python's count() and nltk's count()\n",
        "The count method in the Text object API accepts a word parameter anc counts the number of times the word appears in the text object callin it. There is no difference between this method and the built in str.count(substring) method for strings in python"
      ],
      "metadata": {
        "id": "fpShu_9XlfH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_1 = 'Hello! My name is Savi. Hello!'\n",
        "string_1.count('Hello')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2pohslLldJP",
        "outputId": "bbf8eba6-1c29-44be-9569-f0b6154116d7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = nltk.Text(nltk.word_tokenize(string_1))\n",
        "text_1.count('Hello')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8G_XyiG1mCKx",
        "outputId": "64484ac1-0620-4499-ac86-9f11658e92ab"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Using raw text of at least 5 sentences of your choice from any source (cite the source), save the text into a variable called raw_text. Using NLTK's word tokenizer, tokenize the text into variable 'tokens'. Print the first 10 tokens. "
      ],
      "metadata": {
        "id": "KGT9eYIinvC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "WvEfud2xokLi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text source: Lelouch vi Britannia from Code Geass -- https://tvtropes.org/pmwiki/pmwiki.php/Quotes/CodeGeass\n",
        "raw_text = '''\n",
        "    Attention, entire world! Hear my proclamation: I am Lelouch vi Britannia, Emperor of the Holy Britannian Empire and your only ruler! Schneizel has surrendered to me: as a result of this, I am now in control of both the Damocles and the FLEIJA weapons, and even the Black Knights no longer possess the strength to oppose me now! If anyone dares to oppose my supreme authority, they shall know the devastating power of the FLEIJAs. Those who could oppose my military rule no longer exist! Yes, from this day, from this moment forward, the world belongs to me! Lelouch vi Britannia commands you: Obey me, subjects! OBEY ME, WORLD!\n",
        "'''"
      ],
      "metadata": {
        "id": "rcko9PocmIvj"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(raw_text)\n",
        "tokens[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYr7TkcIoKdu",
        "outputId": "70b2e4c7-d908-42e4-ed06-6fccd6a94184"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Attention', ',', 'entire', 'world', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Using the same raw text, and NLTK's sentence tokenizer sent_tokenize(), perform sentence segmentation and display the sentences. "
      ],
      "metadata": {
        "id": "j5qRN_-Jo1nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize"
      ],
      "metadata": {
        "id": "0Gn2LZFcowNy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(raw_text)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCCc65Sto_Io",
        "outputId": "29992dd0-d752-425d-bc47-e1e58518ddae"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n    Attention, entire world!',\n",
              " 'Hear my proclamation: I am Lelouch vi Britannia, Emperor of the Holy Britannian Empire and your only ruler!',\n",
              " 'Schneizel has surrendered to me: as a result of this, I am now in control of both the Damocles and the FLEIJA weapons, and even the Black Knights no longer possess the strength to oppose me now!',\n",
              " 'If anyone dares to oppose my supreme authority, they shall know the devastating power of the FLEIJAs.',\n",
              " 'Those who could oppose my military rule no longer exist!',\n",
              " 'Yes, from this day, from this moment forward, the world belongs to me!',\n",
              " 'Lelouch vi Britannia commands you: Obey me, subjects!',\n",
              " 'OBEY ME, WORLD!']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.  Using NLTK's PorterStemmer(), write a list comprehension to stem the text. Display the list. "
      ],
      "metadata": {
        "id": "cOrie97WpG50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *"
      ],
      "metadata": {
        "id": "3_-FSZ_TpOvP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "n13Hp2qvpZVN"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[stemmer.stem(t) for t in tokens]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JWvp9h4pC_0",
        "outputId": "32ac0f48-b55b-4187-a877-85bbe25d65b0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['attent',\n",
              " ',',\n",
              " 'entir',\n",
              " 'world',\n",
              " '!',\n",
              " 'hear',\n",
              " 'my',\n",
              " 'proclam',\n",
              " ':',\n",
              " 'i',\n",
              " 'am',\n",
              " 'lelouch',\n",
              " 'vi',\n",
              " 'britannia',\n",
              " ',',\n",
              " 'emperor',\n",
              " 'of',\n",
              " 'the',\n",
              " 'holi',\n",
              " 'britannian',\n",
              " 'empir',\n",
              " 'and',\n",
              " 'your',\n",
              " 'onli',\n",
              " 'ruler',\n",
              " '!',\n",
              " 'schneizel',\n",
              " 'ha',\n",
              " 'surrend',\n",
              " 'to',\n",
              " 'me',\n",
              " ':',\n",
              " 'as',\n",
              " 'a',\n",
              " 'result',\n",
              " 'of',\n",
              " 'thi',\n",
              " ',',\n",
              " 'i',\n",
              " 'am',\n",
              " 'now',\n",
              " 'in',\n",
              " 'control',\n",
              " 'of',\n",
              " 'both',\n",
              " 'the',\n",
              " 'damocl',\n",
              " 'and',\n",
              " 'the',\n",
              " 'fleija',\n",
              " 'weapon',\n",
              " ',',\n",
              " 'and',\n",
              " 'even',\n",
              " 'the',\n",
              " 'black',\n",
              " 'knight',\n",
              " 'no',\n",
              " 'longer',\n",
              " 'possess',\n",
              " 'the',\n",
              " 'strength',\n",
              " 'to',\n",
              " 'oppos',\n",
              " 'me',\n",
              " 'now',\n",
              " '!',\n",
              " 'if',\n",
              " 'anyon',\n",
              " 'dare',\n",
              " 'to',\n",
              " 'oppos',\n",
              " 'my',\n",
              " 'suprem',\n",
              " 'author',\n",
              " ',',\n",
              " 'they',\n",
              " 'shall',\n",
              " 'know',\n",
              " 'the',\n",
              " 'devast',\n",
              " 'power',\n",
              " 'of',\n",
              " 'the',\n",
              " 'fleija',\n",
              " '.',\n",
              " 'those',\n",
              " 'who',\n",
              " 'could',\n",
              " 'oppos',\n",
              " 'my',\n",
              " 'militari',\n",
              " 'rule',\n",
              " 'no',\n",
              " 'longer',\n",
              " 'exist',\n",
              " '!',\n",
              " 'ye',\n",
              " ',',\n",
              " 'from',\n",
              " 'thi',\n",
              " 'day',\n",
              " ',',\n",
              " 'from',\n",
              " 'thi',\n",
              " 'moment',\n",
              " 'forward',\n",
              " ',',\n",
              " 'the',\n",
              " 'world',\n",
              " 'belong',\n",
              " 'to',\n",
              " 'me',\n",
              " '!',\n",
              " 'lelouch',\n",
              " 'vi',\n",
              " 'britannia',\n",
              " 'command',\n",
              " 'you',\n",
              " ':',\n",
              " 'obey',\n",
              " 'me',\n",
              " ',',\n",
              " 'subject',\n",
              " '!',\n",
              " 'obey',\n",
              " 'me',\n",
              " ',',\n",
              " 'world',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Using NLTK's WordNetLemmatizer, write a list comprehension to lemmatize the text. Display the list. \n",
        "\n",
        "### Differences between stems and lemmas (stem - lemma):\n",
        "1. suprem - supreme\n",
        "2. author - authority\n",
        "3. devast - devastating\n",
        "4. militari - military\n",
        "5. thi - this"
      ],
      "metadata": {
        "id": "ykkF_8lwpken"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "7w-xd2PPpfDL"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wn = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "3rfovMyep4ix"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[wn.lemmatize(t) for t in tokens]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_Jd6Hf1p7qz",
        "outputId": "f2fa8532-bacf-4d9f-ba41-a52ea5307fc6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Attention',\n",
              " ',',\n",
              " 'entire',\n",
              " 'world',\n",
              " '!',\n",
              " 'Hear',\n",
              " 'my',\n",
              " 'proclamation',\n",
              " ':',\n",
              " 'I',\n",
              " 'am',\n",
              " 'Lelouch',\n",
              " 'vi',\n",
              " 'Britannia',\n",
              " ',',\n",
              " 'Emperor',\n",
              " 'of',\n",
              " 'the',\n",
              " 'Holy',\n",
              " 'Britannian',\n",
              " 'Empire',\n",
              " 'and',\n",
              " 'your',\n",
              " 'only',\n",
              " 'ruler',\n",
              " '!',\n",
              " 'Schneizel',\n",
              " 'ha',\n",
              " 'surrendered',\n",
              " 'to',\n",
              " 'me',\n",
              " ':',\n",
              " 'a',\n",
              " 'a',\n",
              " 'result',\n",
              " 'of',\n",
              " 'this',\n",
              " ',',\n",
              " 'I',\n",
              " 'am',\n",
              " 'now',\n",
              " 'in',\n",
              " 'control',\n",
              " 'of',\n",
              " 'both',\n",
              " 'the',\n",
              " 'Damocles',\n",
              " 'and',\n",
              " 'the',\n",
              " 'FLEIJA',\n",
              " 'weapon',\n",
              " ',',\n",
              " 'and',\n",
              " 'even',\n",
              " 'the',\n",
              " 'Black',\n",
              " 'Knights',\n",
              " 'no',\n",
              " 'longer',\n",
              " 'posse',\n",
              " 'the',\n",
              " 'strength',\n",
              " 'to',\n",
              " 'oppose',\n",
              " 'me',\n",
              " 'now',\n",
              " '!',\n",
              " 'If',\n",
              " 'anyone',\n",
              " 'dare',\n",
              " 'to',\n",
              " 'oppose',\n",
              " 'my',\n",
              " 'supreme',\n",
              " 'authority',\n",
              " ',',\n",
              " 'they',\n",
              " 'shall',\n",
              " 'know',\n",
              " 'the',\n",
              " 'devastating',\n",
              " 'power',\n",
              " 'of',\n",
              " 'the',\n",
              " 'FLEIJAs',\n",
              " '.',\n",
              " 'Those',\n",
              " 'who',\n",
              " 'could',\n",
              " 'oppose',\n",
              " 'my',\n",
              " 'military',\n",
              " 'rule',\n",
              " 'no',\n",
              " 'longer',\n",
              " 'exist',\n",
              " '!',\n",
              " 'Yes',\n",
              " ',',\n",
              " 'from',\n",
              " 'this',\n",
              " 'day',\n",
              " ',',\n",
              " 'from',\n",
              " 'this',\n",
              " 'moment',\n",
              " 'forward',\n",
              " ',',\n",
              " 'the',\n",
              " 'world',\n",
              " 'belongs',\n",
              " 'to',\n",
              " 'me',\n",
              " '!',\n",
              " 'Lelouch',\n",
              " 'vi',\n",
              " 'Britannia',\n",
              " 'command',\n",
              " 'you',\n",
              " ':',\n",
              " 'Obey',\n",
              " 'me',\n",
              " ',',\n",
              " 'subject',\n",
              " '!',\n",
              " 'OBEY',\n",
              " 'ME',\n",
              " ',',\n",
              " 'WORLD',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write a paragraph outlining:\n",
        "a. your opinion of the functionality of the NLTK library\n",
        "\n",
        "b. your opinion of the code quality of the NLTK library\n",
        "\n",
        "c. a list of ways you may use NLTK in future projects"
      ],
      "metadata": {
        "id": "N6iHgOuwqwzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I think that the NLTK is very functional. It can easily extract words, sentences and other text features with just a few lines of code.\n",
        "# Using other programming languages or even just base Python, these processes would require loops and complex idnex finding for spaces or\n",
        "# quotations, but NLTK makes it easy by just calling a function that only requires one line of code on out end.\n",
        "\n",
        "# I think the code quality of the NLTK library is very nice. Looking through the API and documentation it seems like the code is well\n",
        "# organized and the comments in are informative, making it easy to understand the back end work of the NLTK functions\n",
        "\n",
        "# I can see NLTK being very useful in recommender systems for finding a list of similar words between reviews or descriptions to then\n",
        "# provide recommendations based on. Also NLTK can be used in tone detection by considering only the puncutation returned from a list of\n",
        "# tokens. Lastly, NLTK can be used to perform cleaning on a text dataset, say maybe removing all punctuation or reducing words to lemmas."
      ],
      "metadata": {
        "id": "bY6An9YTqLxO"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qrx_btgStzsW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}